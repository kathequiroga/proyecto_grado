{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e563d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install nltk\n",
    "!pip install --upgrade spacy torch\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7adb7",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e793d1",
   "metadata": {},
   "source": [
    "#### Datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aec623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Acá agregas el archivo que contiene todos los datos para entrenar el modelo\n",
    "train = pd.read_csv('datos_entrenamiento.csv', encoding='ISO-8859-1', delimiter=',') #Revisa si el caracter separador es una coma (,), un punto y coma (;) u otro\n",
    "train = test[[\"title\", \"classification\"]].dropna() #Train debe ser una tabla con dos columnas: \"title\" y \"classification\"\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1973d7",
   "metadata": {},
   "source": [
    "#### Datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef59dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acá agregas el archivo que contiene todos aquellos datos de los cuales quieres saber su categoría\n",
    "#test = pd.read_csv('datos_test.csv', encoding='ISO-8859-1', delimiter=',') #Revisa si el caracter separador es una coma (,), un punto y coma (;) u otro\n",
    "test = pd.read_csv('listArticulos.csv', encoding='ISO-8859-1', delimiter=';') #Acá agregas el archivo que contiene todos aquellos datos de los cuales quieres saber su categoría\n",
    "test = test[[\"title\", \"classification\"]].dropna() #Test debe ser una tabla con dos columnas: \"title\" y \"classification\"\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb919c",
   "metadata": {},
   "source": [
    "## Procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traducir al inglés datos del set de prueba \n",
    "from googletrans import Translator\n",
    "def traducir_texto(texto, idioma):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(texto, dest=idioma)\n",
    "    return translated.text\n",
    "\n",
    "#for i in test.index: # si se quiere traducir también los datos de entrenamiento, cambie test por datos = pd.concat([test, train], axis=1)\n",
    "#  dato = test.loc[i, \"title\"]\n",
    "#  traducido = traducir_texto(dato, \"en\")\n",
    "#  test.loc[i, \"title\"] = traducido\n",
    "\n",
    "datos = pd.concat([test, train])\n",
    "datos = datos.reset_index(drop=True)\n",
    "\n",
    "#Elminar caracteres especiales, puntuación y mayúsculas\n",
    "\n",
    "import re\n",
    "def caracteres_especiales(texto):\n",
    "    muestra = r'[^a-zA-Z0-9\\s]'  # Incluye todo excepto letras, números y espacios en blanco\n",
    "    obslimpia = re.sub(muestra, ' ', texto)\n",
    "    obslimpia = re.sub(r'\\s+', ' ', obslimpia)  \n",
    "    return obslimpia\n",
    "\n",
    "for i in datos.index:\n",
    "  texto = datos.loc[i, \"title\"]\n",
    "  texto = caracteres_especiales(texto) # Quitar caracteres especiales\n",
    "  texto = texto.lower() # Estandarizar a minúsculas\n",
    "  datos.loc[i, \"title\"] = texto\n",
    "    \n",
    "\n",
    "# Eliminar stop words y lematizar palabras\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #Modelo de lenguaje en inglés de spaCy\n",
    "for i in datos.index:\n",
    "  doc = nlp(datos.loc[i, \"title\"])\n",
    "  palabras = [token.lemma_ for token in doc if not token.is_stop] \n",
    "  texto = ' '.join(palabras)\n",
    "  datos.loc[i, \"title\"] = texto\n",
    "\n",
    "\n",
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(texto):\n",
    "    obs = [stemmer.stem(token.lemma_) for token in nlp(texto)]\n",
    "    return \" \".join(obs)\n",
    "\n",
    "for i in datos.index:\n",
    "  datos.loc[i, \"title\"] = stemming(datos.loc[i, \"title\"])\n",
    "\n",
    "#Limpieza extra\n",
    "for i in datos.index:\n",
    "  datos.loc[i, \"title\"] = re.sub(r'\\s+', ' ', datos.loc[i, \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004706f",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "titulo = datos[\"title\"]\n",
    "vectorizer = CountVectorizer(min_df=2) #BoW con aquellas palabras que aparecen mínimo 2 veces en el total de observacions\n",
    "matriz_bow = vectorizer.fit_transform(titulo)\n",
    "bow = pd.DataFrame(matriz_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos nuevamente en entrenamiento y prueba\n",
    "X_test = bow.iloc[:len(test)]\n",
    "X_train = bow.iloc[len(test):]\n",
    "y_test = datos.iloc[:len(test), 1]\n",
    "y_train = datos.iloc[len(test):, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919de9d",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b110197",
   "metadata": {},
   "source": [
    "#### Si queremos predecir con hasta dos categorías como respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión logística one-vs-rest\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clases = ['agricultural systems', 'energy systems', 'financial engineering systems', 'health systems', 'production systems', 'sustainable systems', 'transportation systems', 'urban systems']\n",
    "\n",
    "modelrl = LogisticRegression(C= 0.1, max_iter= 100, penalty= 'l2', solver= 'lbfgs', multi_class='ovr')\n",
    "modelrl.fit(X_train, y_train)\n",
    "y_probs = modelrl.predict_proba(X_test)\n",
    "\n",
    "yhat_threshold = []\n",
    "\n",
    "for indice, prob in enumerate(y_probs): #Recorro las probabilidades obtenidas para cada observación\n",
    "    clases_sobre_threshold = np.where(prob > 0.8*np.max(prob))[0] #Agrego la nueva regla de decisión\n",
    "    if len(clases_sobre_threshold) == 0:\n",
    "        clases_sobre_threshold = [np.argmax(prob)]\n",
    "\n",
    "    #Si hay más de 2 categorías elegidas, selecciona aquellas dos con más probabilidad\n",
    "    if len(clases_sobre_threshold) > 2:\n",
    "        indices_ordenados = np.argsort(prob)[::-1]  \n",
    "        clases_sobre_threshold = indices_ordenados[:2]\n",
    "\n",
    "    yhat_threshold.append(clases_sobre_threshold)\n",
    "    cat_correcta = y2_test[indice]\n",
    "    convers = clases.index(cat_correcta)\n",
    "    \n",
    "    seleccionadas = []\n",
    "    for i in clases_sobre_threshold:\n",
    "        seleccionadas.append(clases[i])\n",
    "    \n",
    "    yhat_threshold.append(seleccionadas)\n",
    "\n",
    "print(yhat_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargar los resultados como un archivo csv\n",
    "yhat_threshold.to_csv('resultados_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcd950",
   "metadata": {},
   "source": [
    "#### Si queremos predecir con solo una categoría de respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión logística one-vs-rest\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelrl = LogisticRegression(C= 0.1, max_iter= 100, penalty= 'l2', solver= 'lbfgs', multi_class='ovr')\n",
    "modelrl.fit(X_train, y_train)\n",
    "y_pred = modelrl.predict(X_test)\n",
    "\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
